# A Paper List of Yang Liu
## Loco-manipulation
- arXiv 2025, **HoST**:Learning Humanoid Standing-up Control Across Diverse Postures, [Website](https://taohuang13.github.io/humanoid-standingup.github.io/)
- IROS 2024, **human2humanoid**: Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation, [Website](https://human2humanoid.com/)
- arXiv 2025, **HugWBC**: A Unified and General Humanoid Whole-Body Controller for Fine-Grained Locomotion, [Website](https://hugwbc.github.io/)
- arXiv 2025, **Embrace Collisions**: Humanoid Shadowing for Deployable Contact-Agnostics Motions, [Website](https://project-instinct.github.io/)
- arXiv 2024,**Exbody2**: Advanced Expressive Humanoid Whole-Body Control, [Website](https://exbody2.github.io/)
- RSS 2024, **Exbody**: Expressive Whole-Body Control for Humanoid Robots, [Website](https://expressive-humanoid.github.io/)
- Humanoids 2023, Deep Imitation Learning for Humanoid Loco-manipulation through Human Teleoperation, [Website](https://ut-austin-rpl.github.io/TRILL/)
- arXiv 2025, **ASAP**: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills, [Website](https://agile.human2humanoid.com/)
- CoRL 2024, **VBC**: Visual Whole-Body Control for Legged Loco-Manipulation, [Website](https://wholebody-b1.github.io/)
- CoRL 2022, **Deep Whole-Body Control**: Learning a Unified Policy for Manipulation and Locomotion, [Website](https://manipulation-locomotion.github.io/)
- ICRA 2023, Learning Whole-body Manipulation for Quadrupedal Robot, [arXiv](https://arxiv.org/abs/2308.16820)
## Locomotion
### Humanoid
- arXiv 2025, **BeamDojo**: Learning Agile Humanoid Locomotion on Sparse Footholds, [Website](https://why618188.github.io/beamdojo/)
- arXiv 2025, **VB-Com**: Learning Vision-Blind Composite Humanoid Locomotion Against Deficient Perception, [Website](https://renjunli99.github.io/vbcom.github.io/)
- CoRL 2024, **Humanoid Parkour Learning**, [Website](https://humanoid4parkour.github.io/)
- arXiv 2024, **PIM**: Learning Humanoid Locomotion with Perceptive Internal Model, [Website](https://junfeng-long.github.io/PIM/)
- Science Robotics 2024, Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning, [arxiv](https://arxiv.org/abs/2304.13653)
### Quadruped
- arXiv 2024,**MOVE**: Multi-skill Omnidirectional Legged Locomotion with Limited View in 3D Environments, [arXiv](https://arxiv.org/abs/2412.03353)
- ICLR 2024, **HIMLOCO**: Hybrid Internal Model for Legged Locomotion, [Website](https://junfeng-long.github.io/HIMLoco/)
- CoRL 2022, **Walk These Ways**: Tuning Robot Control for Generalization with Multiplicity of Behavior, [Website]()
- arXiv 2024, **Walking with Terrain Reconstruction**: Learning to Traverse Risky Sparse Footholds, [arXiv](https://arxiv.org/pdf/2409.15692)
- arXiv 2024, **PIE**: Parkour with Implicit-Explicit Learning Framework for Legged Robots, [arXiv](https://arxiv.org/pdf/2408.13740..)
- arXiv 2024, **Helpful DoggyBot**: Open-World Object Fetching using Legged Robots and Vision-Language Models, [Website](https://helpful-doggybot.github.io/)
- arXiv 2023, **Anymal parkour**: Learning agile navigation for quadrupedal robots, [arXiv](https://arxiv.org/abs/2306.14874)
- arXiv 2024, **Not Only Rewards But Also Constraints**:Applications on Legged Robot Locomotion, [arXiv](https://arxiv.org/pdf/2308.12517)
- arXiv 2023, **DreamWaQ**: Learning Robust Quadrupedal Locomotion With ImplicitTerrain Imagination via Deep Reinforcement Learning, [arXiv](https://arxiv.org/abs/2301.10602)
- RSS 2023, Robust Recovery Motion Control for Quadrupedal Robots via Learned Terrain Imagination, [Webiste](https://sites.google.com/view/dreamriser)
- Nature Machine Intelligence 2024, Lifelike Agility and Play in Quadrupedal Robots using Reinforcement Learning and Generative Pre-trained Models, [Website](https://tencent-roboticsx.github.io/lifelike-agility-and-play/)
- CoRL 2021, Minimizing Energy Consumption Leads to the Emergence of Gaits in Legged Robots, [Webiste](https://energy-locomotion.github.io/)
- CoRL 2023, Robot Parkour Learning, [Website](https://robot-parkour.github.io/)
- ICRA 2024, Extreme Parkour with Legged Robots, [Webisite](https://extreme-parkour.github.io/)
- ScienceDirect 2021, Development of quadruped walking robots: A review, [ScienceDirect](https://www.sciencedirect.com/science/article/pii/S2090447920302501)
- ACM SIGGRAPH 2021, **AMP**: Adversarial Motion Priors for Stylized Physics-Based CharacterControl, [Website](https://xbpeng.github.io/projects/AMP/index.html)
- Science Robotics 2019, Learning agile and dynamic motor skills for legged robots, [Video](https://youtu.be/aTDkYFZFWug?si=uOz0P2ErlVum0TO0) / [arXiv](https://arxiv.org/abs/1901.08652)
- Science Robotics 2020, Learning Quadrupedal Locomotion over Challenging Terrain, [Website](https://leggedrobotics.github.io/rl-blindloco/)
- RSS 2021, **RMA**: Rapid Motor Adaptation for Legged Robots, [Website](https://ashish-kmr.github.io/rma-legged-robots/)
- CoRL 2022, Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning, [Website](https://leggedrobotics.github.io/legged_gym/) /  [arXiv](https://arxiv.org/abs/2109.11978)
- RA-L 2023, Learning Robust and Agile Legged Locomotion Using Adversarial Motion Priors, [bilibili](https://www.bilibili.com/video/BV1nM4y177rY/)

## Manipulation

- CoRL 2024, **OpenVLA**: An Open-Source Vision-Language-Action Model, [Webiste](https://openvla.github.io/)
- arXiv 2024, **EgoMimic**: Scaling Imitation Learning through Egocentric Video, [Website](https://egomimic.github.io/)


- CoRL 2024, **Open-TeleVision**:Teleoperation with Immersive Active Visual Feedback, [Website](https://robot-tv.github.io/)
- RSS 2023, **Diffusion Policy** Visuomotor Policy Learning via Action Diffusion, [Website](https://diffusion-policy.cs.columbia.edu/)
- arXiv 2024, **Catch It!**:Learning to Catch in Flight with Mobile Dexterous Hands, [Website](https://mobile-dex-catch.github.io/)
- CoRL 2024, **UMI on Legs**: Making Manipulation Policies Mobile with Manipulation-Centric Whole-body Controllers, [Website](https://umi-on-legs.github.io/)

- arXiv 2024, **Survey**: Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis, [arXiv](https://arxiv.org/abs/2312.08782)

- arXiv 2024, **GVL**:Vision Language Models are In-Context Value Learners, [Website](https://generative-value-learning.github.io/#online-demo)
- ECCV 2024, **COPL**:Visual Grounding for Object-Level Generalization in Reinforcement Learning, [arXiv](https://arxiv.org/abs/2408.01942)
- ICRA 2024, **Pedipulate**: Enabling Manipulation Skills using a Quadruped Robotâ€™s Leg, [Website](https://sites.google.com/leggedrobotics.com/pedipulate)

- RSS 2023, **ALOHA**: Learning Fine-Grained Bimanual Manipulation with  Low-Cost Hardware, [Website](https://tonyzhaozh.github.io/aloha/)
- CoRL 2022, **Perceiver-Actor**: A Multi-Task Transformer for Robotic Manipulation, [Website](https://peract.github.io/)
- ICML 2021, **CLIP**: Learning Transferable Visual Models From Natural Language Supervision, [arXiv](https://arxiv.org/abs/2103.00020)
- ICRA 2024, **Talk Through It**: End User Directed Manipulation Learning, [Website](https://talk-through-it.github.io/)

- arXiv 2024, **Open X-Embodiment**: Robotic Learning Datasets and RT-X Models, [arXiv](https://arxiv.org/pdf/2310.08864)

- arXiv 2024, Do As I Can, Not As I Say:Grounding Language in Robotic Affordances, [arXiv](https://arxiv.org/abs/2204.01691)
- RSS 2024, **RVT-2**: Learning Precise Manipulation from Few Examples, [Website](https://robotic-view-transformer-2.github.io/)


- CoRL 2020, **Transporter Networks**: Rearranging the Visual World for Robotic Manipulation, [Webiste](https://transporternets.github.io/)

- arXiv 2016, A Connection Between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models, [arXiv](https://arxiv.org/abs/1611.03852)
## Navigation
- CVPR 2023, **CoWs on Pasture**:Baselines and Benchmarks for Language-Driven Zero-Shot Object Navigation, [Website](https://cow.cs.columbia.edu/)
- RSS 2024, **GOAT**: GO to Any Thing, [Website](https://theophilegervet.github.io/projects/goat/)
- CoRL 2023, Seeing-Eye Quadruped Navigation with Force Responsive Locomotion Control, [Website](https://bu-air-lab.github.io/guide_dog/)


