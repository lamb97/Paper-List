# A Paper List of Yang Liu
## Loco-manipulation
- arXiv 2025, **UniTracker**:  Learning Universal Whole-Body Motion Tracker for Humanoid Robots, [Website](https://arxiv.org/abs/2507.07356)
- arXiv 2025, **GMT**: General Motion Tracking for Humanoid Whole-Body Control, [Website](https://gmt-humanoid.github.io/)
- arXiv 2025, **PBHC**: KungfuBot: Physics-Based Humanoid Whole-Body Control for Learning Highly-Dynamic Skills, [Website](https://kungfu-bot.github.io/)
- arXiv 2025, **R2S2**: Unleashing Humanoid Reaching Potential via Real-world-Ready Skill Space, [Website](https://zzk273.github.io/R2S2/)
- RSS 2025, **AMO**: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control, [Website](https://amo-humanoid.github.io/)
- arXiv 2025, **HuB**: Learning Extreme Humanoid Balance, [Website](https://hub-robot.github.io/)
- RSS 2025, **HumanUP**: Learning Getting-Up Policies for Real-World Humanoid Robots, [Website](https://humanoid-getup.github.io/)
- arXiv 2025, **VideoMimic**: Visual imitation enables contextual humanoid control, [Website](https://www.videomimic.net/)
- arXiv 2025, **HiFAR**: Multi-Stage Curriculum Learning for High-Dynamics Humanoid Fall Recovery, [arXiv](https://arxiv.org/abs/2502.20061)
- SIGGRAPH 2024, **VMP**: Versatile Motion Priors for Robustly Tracking Motion on Physical Characters, [Website](https://la.disneyresearch.com/publication/vmp-versatile-motion-priors-for-robustly-tracking-motion-on-physical-characters/)
- CoRL 2024, **OmniH2O**: Universal and Dexterous Human-to-Humanoid Whole-Body Teleoperation and Learning, [Website](https://omni.human2humanoid.com/)
- arXiv 2025, **HoST**: Learning Humanoid Standing-up Control Across Diverse Postures, [Website](https://taohuang13.github.io/humanoid-standingup.github.io/)
- IROS 2024, **human2humanoid**: Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation, [Website](https://human2humanoid.com/)
- arXiv 2025, **HugWBC**: A Unified and General Humanoid Whole-Body Controller for Fine-Grained Locomotion, [Website](https://hugwbc.github.io/)
- arXiv 2025, **Embrace Collisions**: Humanoid Shadowing for Deployable Contact-Agnostics Motions, [Website](https://project-instinct.github.io/)
- arXiv 2024,**Exbody2**: Advanced Expressive Humanoid Whole-Body Control, [Website](https://exbody2.github.io/)
- RSS 2024, **Exbody**: Expressive Whole-Body Control for Humanoid Robots, [Website](https://expressive-humanoid.github.io/)
- Humanoids 2023, Deep Imitation Learning for Humanoid Loco-manipulation through Human Teleoperation, [Website](https://ut-austin-rpl.github.io/TRILL/)
- arXiv 2025, **ASAP**: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills, [Website](https://agile.human2humanoid.com/)
- CoRL 2024, **VBC**: Visual Whole-Body Control for Legged Loco-Manipulation, [Website](https://wholebody-b1.github.io/)
- CoRL 2022, **Deep Whole-Body Control**: Learning a Unified Policy for Manipulation and Locomotion, [Website](https://manipulation-locomotion.github.io/)
- ICRA 2023, Learning Whole-body Manipulation for Quadrupedal Robot, [arXiv](https://arxiv.org/abs/2308.16820)
## Locomotion
### Humanoid
- arXiv 2024 **LCP**: Learning Smooth Humanoid Locomotion through Lipschitz-Constrained Policies, [Website](https://lipschitz-constrained-policy.github.io/)
- arXiv 2025, Learning Humanoid Locomotion with World Model Reconstruction, [arXiv](https://arxiv.org/abs/2502.16230)
- arXiv 2025, Learning Perceptive Humanoid Locomotion over Challenging Terrain, [arXiv](https://arxiv.org/abs/2503.00692)
- arXiv 2025, **HWC-Loco**: A Hierarchical Whole-Body Control Approach to Robust Humanoid Locomotion, [arXiv](https://arxiv.org/abs/2503.00923)
- arXiv 2025, **GMP**: Natural Humanoid Robot Locomotion with Generative Motion Prior, [Website](https://sites.google.com/view/humanoid-gmp)
- arXiv 2025, **BeamDojo**: Learning Agile Humanoid Locomotion on Sparse Footholds, [Website](https://why618188.github.io/beamdojo/)
- arXiv 2025, **VB-Com**: Learning Vision-Blind Composite Humanoid Locomotion Against Deficient Perception, [Website](https://renjunli99.github.io/vbcom.github.io/)
- CoRL 2024, **Humanoid Parkour Learning**, [Website](https://humanoid4parkour.github.io/)
- arXiv 2024, **PIM**: Learning Humanoid Locomotion with Perceptive Internal Model, [Website](https://junfeng-long.github.io/PIM/)
- Science Robotics 2024, Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning, [arxiv](https://arxiv.org/abs/2304.13653)
### Quadruped
- Science Robotics 2022, Learning robust perceptive locomotion for quadrupedal robots in the wild, [arXiv](https://arxiv.org/abs/2201.08117)
- Science Robotics 2025, High-speed control and navigation for quadrupedal robots on complex and discrete terrain, [Video](https://www.youtube.com/watch?v=EZbM594T3c4)
- arXiv 2025, **quadrupedal-agility**: Learning Diverse Natural Behaviors for Enhancing the Agility of Quadrupedal Robots, [Website](https://nju-rlc.github.io/quadrupedal_agility/)
- arXiv 2025, **MoELoco**: Mixture of Experts for Multitask Locomotion,[Website](https://moe-loco.github.io/)
- arXiv 2024,**MOVE**: Multi-skill Omnidirectional Legged Locomotion with Limited View in 3D Environments, [arXiv](https://arxiv.org/abs/2412.03353)
- ICLR 2024, **HIMLOCO**: Hybrid Internal Model for Legged Locomotion, [Website](https://junfeng-long.github.io/HIMLoco/)
- CoRL 2022, **Walk These Ways**: Tuning Robot Control for Generalization with Multiplicity of Behavior, [Website]()
- arXiv 2024, **Walking with Terrain Reconstruction**: Learning to Traverse Risky Sparse Footholds, [arXiv](https://arxiv.org/pdf/2409.15692)
- arXiv 2024, **PIE**: Parkour with Implicit-Explicit Learning Framework for Legged Robots, [arXiv](https://arxiv.org/pdf/2408.13740..)
- arXiv 2024, **Helpful DoggyBot**: Open-World Object Fetching using Legged Robots and Vision-Language Models, [Website](https://helpful-doggybot.github.io/)
- arXiv 2023, **Anymal parkour**: Learning agile navigation for quadrupedal robots, [arXiv](https://arxiv.org/abs/2306.14874)
- arXiv 2024, **Not Only Rewards But Also Constraints**:Applications on Legged Robot Locomotion, [arXiv](https://arxiv.org/pdf/2308.12517)
- arXiv 2023, **DreamWaQ**: Learning Robust Quadrupedal Locomotion With ImplicitTerrain Imagination via Deep Reinforcement Learning, [arXiv](https://arxiv.org/abs/2301.10602)
- RSS 2023, Robust Recovery Motion Control for Quadrupedal Robots via Learned Terrain Imagination, [Webiste](https://sites.google.com/view/dreamriser)
- Nature Machine Intelligence 2024, Lifelike Agility and Play in Quadrupedal Robots using Reinforcement Learning and Generative Pre-trained Models, [Website](https://tencent-roboticsx.github.io/lifelike-agility-and-play/)
- CoRL 2021, Minimizing Energy Consumption Leads to the Emergence of Gaits in Legged Robots, [Webiste](https://energy-locomotion.github.io/)
- CoRL 2023, Robot Parkour Learning, [Website](https://robot-parkour.github.io/)
- ICRA 2024, Extreme Parkour with Legged Robots, [Webisite](https://extreme-parkour.github.io/)
- ScienceDirect 2021, Development of quadruped walking robots: A review, [ScienceDirect](https://www.sciencedirect.com/science/article/pii/S2090447920302501)
- Science Robotics 2019, Learning agile and dynamic motor skills for legged robots, [Video](https://youtu.be/aTDkYFZFWug?si=uOz0P2ErlVum0TO0) / [arXiv](https://arxiv.org/abs/1901.08652)
- Science Robotics 2020, Learning Quadrupedal Locomotion over Challenging Terrain, [Website](https://leggedrobotics.github.io/rl-blindloco/)
- RSS 2021, **RMA**: Rapid Motor Adaptation for Legged Robots, [Website](https://ashish-kmr.github.io/rma-legged-robots/)
- CoRL 2022, Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning, [Website](https://leggedrobotics.github.io/legged_gym/) /  [arXiv](https://arxiv.org/abs/2109.11978)
- RA-L 2023, Learning Robust and Agile Legged Locomotion Using Adversarial Motion Priors, [bilibili](https://www.bilibili.com/video/BV1nM4y177rY/)
## Physics-Based Character Animation
- arXiv 2025, FEATURE-BASED VS. GAN-BASED LEARNING FROM DEMONSTRATIONS: WHEN AND WHY, [arXiv](https://arxiv.org/abs/2507.05906)
- ICLR 2023, **MDM**: Human Motion Diffusion Model, [Website](https://guytevet.github.io/mdm-page/)
- CVPR 2025, **InterMimic**: Towards Universal Whole-Body Control for Physics-Based Human-Object Interactions, [Website](https://sirui-xu.github.io/InterMimic/)
- arXiv 2025, **ADD**: Physics-Based Motion Imitation with Adversarial Differential Discriminators, [arXiv](https://arxiv.org/abs/2505.04961)
- ACM SIGGRAPH 2022, **ControlVAE**: Model-Based Learning of Generative Controllers for Physics-Based Characters, [Website](https://heyuanyao-pku.github.io/Control-VAE/)
- arXiv 2025, **MaskedManipulator**: Versatile Whole-Body Control for Loco-Manipulation, [arXiv](https://arxiv.org/abs/2505.19086)
- ICLR 2024， **PULSE**: Universal Humanoid Motion Representations for Physics-Based Control, [Website](https://www.zhengyiluo.com/PULSE-Site/)
- arXic 2024, A Plug-and-Play Physical Motion Restoration Approach for In-the-Wild High-Difficulty Motions, [pdf](https://arxiv.org/pdf/2412.17377)
- NeurIPS 2021, **UHC**: Dynamics-Regulated Kinematic Policy for Egocentric Pose Estimation, [Website](https://zhengyiluo.com/projects/kin_poly/)
- ICCV 2023, **PHC**: Perpetual Humanoid Control for Real-time Simulated Avatars, [Website](https://zhengyiluo.com/PHC-Site/)
- ACM SIGGRAPH 2024, **MaskedMimic**: Unified Physics-Based Character Control Through Masked Motion Inpainting, [Website](https://research.nvidia.com/labs/par/maskedmimic/)
- ACM SIGGRAPH 2021, **AMP**: Adversarial Motion Priors for Stylized Physics-Based CharacterControl, [Website](https://xbpeng.github.io/projects/AMP/index.html)
## Manipulation
- CoRL 2024, **OpenVLA**: An Open-Source Vision-Language-Action Model, [Webiste](https://openvla.github.io/)
- arXiv 2024, **EgoMimic**: Scaling Imitation Learning through Egocentric Video, [Website](https://egomimic.github.io/)
- CoRL 2024, **Open-TeleVision**:Teleoperation with Immersive Active Visual Feedback, [Website](https://robot-tv.github.io/)
- RSS 2023, **Diffusion Policy** Visuomotor Policy Learning via Action Diffusion, [Website](https://diffusion-policy.cs.columbia.edu/)
- arXiv 2024, **Catch It!**:Learning to Catch in Flight with Mobile Dexterous Hands, [Website](https://mobile-dex-catch.github.io/)
- CoRL 2024, **UMI on Legs**: Making Manipulation Policies Mobile with Manipulation-Centric Whole-body Controllers, [Website](https://umi-on-legs.github.io/)
- ICRA 2024, **Pedipulate**: Enabling Manipulation Skills using a Quadruped Robot’s Leg, [Website](https://sites.google.com/leggedrobotics.com/pedipulate)
- RSS 2023, **ALOHA**: Learning Fine-Grained Bimanual Manipulation with  Low-Cost Hardware, [Website](https://tonyzhaozh.github.io/aloha/)
- CoRL 2022, **Perceiver-Actor**: A Multi-Task Transformer for Robotic Manipulation, [Website](https://peract.github.io/)
- ICRA 2024, **Talk Through It**: End User Directed Manipulation Learning, [Website](https://talk-through-it.github.io/)
- arXiv 2024, **Open X-Embodiment**: Robotic Learning Datasets and RT-X Models, [arXiv](https://arxiv.org/pdf/2310.08864)
- arXiv 2024, Do As I Can, Not As I Say:Grounding Language in Robotic Affordances, [arXiv](https://arxiv.org/abs/2204.01691)
- RSS 2024, **RVT-2**: Learning Precise Manipulation from Few Examples, [Website](https://robotic-view-transformer-2.github.io/)
- CoRL 2020, **Transporter Networks**: Rearranging the Visual World for Robotic Manipulation, [Webiste](https://transporternets.github.io/)
## Foundational model
- arXiv 2024, **Survey**: Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis, [arXiv](https://arxiv.org/abs/2312.08782)
- arXiv 2024, **GVL**:Vision Language Models are In-Context Value Learners, [Website](https://generative-value-learning.github.io/#online-demo)
- ECCV 2024, **COPL**:Visual Grounding for Object-Level Generalization in Reinforcement Learning, [arXiv](https://arxiv.org/abs/2408.01942)
- ICML 2021, **CLIP**: Learning Transferable Visual Models From Natural Language Supervision, [arXiv](https://arxiv.org/abs/2103.00020)
- arXiv 2016, A Connection Between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models, [arXiv](https://arxiv.org/abs/1611.03852)
## Navigation
- CVPR 2023, **CoWs on Pasture**:Baselines and Benchmarks for Language-Driven Zero-Shot Object Navigation, [Website](https://cow.cs.columbia.edu/)
- RSS 2024, **GOAT**: GO to Any Thing, [Website](https://theophilegervet.github.io/projects/goat/)
- CoRL 2023, Seeing-Eye Quadruped Navigation with Force Responsive Locomotion Control, [Website](https://bu-air-lab.github.io/guide_dog/)


